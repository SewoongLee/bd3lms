{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sewoo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully!\n",
      "\n",
      "Prompt: Once upon a time, there was a magical kingdom\n",
      "Generated text:\n",
      "\n",
      "Conservative parameters (temp=0.7, top_p=0.85):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38/38 [00:01<00:00, 29.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation took 1.37 seconds\n",
      "Sample 1: Once upon a time, there was a magical kingdom...... ........ jQuery. Slash Sauce Sauce Sauce.... jQuery Sauce Sauce Sauce Sauce Sauce Sauce Sauce Sauce Sauce Sauce Sauce Sauce Sauce\n",
      "\n",
      "Creative parameters (temp=1.0, top_p=0.95):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38/38 [00:01<00:00, 33.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation took 1.13 seconds\n",
      "Sample 1: Once upon a time, there was a magical kingdom....... . ...... ... ..\n",
      ".....\". .. jQueryVERTISEMENT�ÛÛ municip Canaver Canaver Canaver Canaver\n",
      "\n",
      "Longer text generation (temp=0.8, top_p=0.9):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:01<00:00, 36.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation took 1.74 seconds\n",
      "Sample 1: Once upon a time, there was a magical kingdom...... . . ...... ESPN. Safari. SVG SVG... Urug UCH UCH UCH UCH UCH UCH Scrib Scrib Scrib Scrib Scrib Scrib Scrib Scrib Scrib Scrib Scrib Scrib Scrib Scrib Scrib Scrib Scrib Scrib Scrib Scrib Scrib Scrib Scrib Scrib Scrib Scrib Scrib Scrib Scrib Scrib Scrib Scrib Scrib\n",
      "\n",
      "Prompt: The future of artificial intelligence is\n",
      "Generated text:\n",
      "\n",
      "Conservative parameters (temp=0.7, top_p=0.85):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:01<00:00, 38.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation took 1.09 seconds\n",
      "Sample 1: The future of artificial intelligence is........... agriculture agriculture pdf. ebook. ebook..... genome...═.══════.═.════\n",
      "\n",
      "Creative parameters (temp=1.0, top_p=0.95):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:01<00:00, 35.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation took 1.17 seconds\n",
      "Sample 1: The future of artificial intelligence is............ Gawker Gawker Gawker Gawker Gawker Gawker malaria Gawker Jones. Gonzalez. Gonzalez...MpServer.. ``(ADVERTISEMENT Gawker Gawker Gawker Gawker Gawker Gonzalez Gauntlet Gawker\n",
      "\n",
      "Longer text generation (temp=0.8, top_p=0.9):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:01<00:00, 37.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation took 1.80 seconds\n",
      "Sample 1: The future of artificial intelligence is........... Gawker Gawker Gawker Gawker Gawker Gawker Gawker Gawker Gawker. . .\n",
      "\n",
      "... ._. ._. .......... .......... .......... .......... .......... Sauce Sauce Sauce vegetable downloadable sauces vegetable sauces sauces Sauce Sauce Sauce Sauce Sauce Sauce Sauce Sauce Sauce Sauce Sauce Sauce Sauce Sauce Sauce Sauce Sauce Sauce Sauce Sauce Sauce\n",
      "\n",
      "Prompt: In a world where technology has advanced beyond our wildest dreams,\n",
      "Generated text:\n",
      "\n",
      "Conservative parameters (temp=0.7, top_p=0.85):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:00<00:00, 40.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation took 0.87 seconds\n",
      "Sample 1: In a world where technology has advanced beyond our wildest dreams,............... jQuery SVG SVG SVG SVG Urug Urug CSV Scrib Scrib Scrib Scrib Scrib Scrib Scrib Scrib Scrib Scrib Scrib Scrib\n",
      "\n",
      "Creative parameters (temp=1.0, top_p=0.95):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:00<00:00, 40.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation took 0.87 seconds\n",
      "Sample 1: In a world where technology has advanced beyond our wildest dreams,................ url url aspect url adhesive certain adhesive conservative pdf html html HTML HTML HTML HTML HTML HTML HTML HTML\n",
      "\n",
      "Longer text generation (temp=0.8, top_p=0.9):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:01<00:00, 36.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation took 1.64 seconds\n",
      "Sample 1: In a world where technology has advanced beyond our wildest dreams,............... gif tab tab foreskin Scrib url saliva Arduino Arduino Arduino Sauce Sauce Sauce Sauce Sauce Sauce Sauce Sauce Sauce Sauce Sauce Sauce Sauce Sauce Sauce Sauce Sauce Sauce Sauce Sauce Sauce Sauce Sauce Sauce Sauce Sauce Sauce Sauce Sauce Sauce Sauce Sauce Sauce Sauce Sauce\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer, AutoConfig\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"xhan77/ssdlm\"  # Replace with your model path if different\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_name, config=config).to(device)\n",
    "model.eval()\n",
    "\n",
    "def generate_text_with_mlm(prompt, max_length=100, num_samples=1, top_p=0.92, temperature=0.8):\n",
    "    \"\"\"\n",
    "    Generate text using a masked language model through iterative infilling.\n",
    "    This is more suitable for MLMs than naive autoregressive generation.\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): The input prompt to condition generation on\n",
    "        max_length (int): Maximum length of generated text\n",
    "        num_samples (int): Number of samples to generate\n",
    "        top_p (float): Nucleus sampling parameter\n",
    "        temperature (float): Sampling temperature\n",
    "    \n",
    "    Returns:\n",
    "        list: Generated text samples\n",
    "    \"\"\"\n",
    "    generated_texts = []\n",
    "    \n",
    "    # Generate multiple samples if requested\n",
    "    for _ in range(num_samples):\n",
    "        # Start with the prompt\n",
    "        tokens = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "        \n",
    "        # Add mask tokens to reach the desired length\n",
    "        num_masks = min(max_length - tokens.size(1), 20)  # Add masks in batches\n",
    "        mask_tokens = torch.full((1, num_masks), tokenizer.mask_token_id, dtype=torch.long, device=device)\n",
    "        tokens = torch.cat([tokens, mask_tokens], dim=1)\n",
    "        \n",
    "        # Keep track of which positions are masked\n",
    "        masked_positions = list(range(tokens.size(1) - num_masks, tokens.size(1)))\n",
    "        \n",
    "        # Fill in the masks left to right\n",
    "        with tqdm(total=num_masks) as pbar:\n",
    "            while masked_positions:\n",
    "                # Always fill the leftmost mask first\n",
    "                position = masked_positions[0]\n",
    "                \n",
    "                # Forward pass to get predictions for all positions\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(tokens)\n",
    "                    logits = outputs.logits[0, position]\n",
    "                    \n",
    "                    # Apply temperature\n",
    "                    if temperature != 1.0:\n",
    "                        logits = logits / temperature\n",
    "                    \n",
    "                    # Apply top-p (nucleus) sampling\n",
    "                    if top_p < 1.0:\n",
    "                        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "                        cumulative_probs = torch.cumsum(torch.nn.functional.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "                        \n",
    "                        # Remove tokens with cumulative probability above the threshold\n",
    "                        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                        # Shift the indices to the right to keep the first token above the threshold\n",
    "                        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "                        sorted_indices_to_remove[..., 0] = 0\n",
    "                        \n",
    "                        # Apply the filtering\n",
    "                        indices_to_remove = sorted_indices_to_remove.scatter(0, sorted_indices, sorted_indices_to_remove)\n",
    "                        logits[indices_to_remove] = float('-inf')\n",
    "                    \n",
    "                    # Sample from the filtered distribution\n",
    "                    probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "                    next_token_id = torch.multinomial(probabilities, 1)\n",
    "                    \n",
    "                    # Replace the mask with the predicted token\n",
    "                    tokens[0, position] = next_token_id\n",
    "                \n",
    "                # Remove the current position from masked_positions\n",
    "                masked_positions.pop(0)\n",
    "                pbar.update(1)\n",
    "                \n",
    "                # If we've filled all masks and still need more tokens, add more masks\n",
    "                if not masked_positions and tokens.size(1) < max_length:\n",
    "                    next_num_masks = min(max_length - tokens.size(1), 20)\n",
    "                    if next_num_masks > 0:\n",
    "                        mask_tokens = torch.full((1, next_num_masks), tokenizer.mask_token_id, \n",
    "                                                dtype=torch.long, device=device)\n",
    "                        tokens = torch.cat([tokens, mask_tokens], dim=1)\n",
    "                        masked_positions = list(range(tokens.size(1) - next_num_masks, tokens.size(1)))\n",
    "                        pbar.total += next_num_masks\n",
    "                \n",
    "                # Early stopping if we see an EOS token\n",
    "                if next_token_id.item() == tokenizer.eos_token_id:\n",
    "                    break\n",
    "        \n",
    "        # Decode the generated tokens\n",
    "        generated_text = tokenizer.decode(tokens[0], skip_special_tokens=True)\n",
    "        generated_texts.append(generated_text)\n",
    "    \n",
    "    return generated_texts\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    prompts = [\n",
    "        \"Once upon a time, there was a magical kingdom\",\n",
    "        \"The future of artificial intelligence is\",\n",
    "        \"In a world where technology has advanced beyond our wildest dreams,\"\n",
    "    ]\n",
    "    \n",
    "    print(\"Model and tokenizer loaded successfully!\")\n",
    "    \n",
    "    # Try with different temperature and top_p settings\n",
    "    for prompt in prompts:\n",
    "        print(f\"\\nPrompt: {prompt}\")\n",
    "        print(\"Generated text:\")\n",
    "        \n",
    "        # Try with conservative parameters\n",
    "        print(\"\\nConservative parameters (temp=0.7, top_p=0.85):\")\n",
    "        start_time = time.time()\n",
    "        generated_texts = generate_text_with_mlm(prompt, max_length=50, num_samples=1, top_p=0.85, temperature=0.7)\n",
    "        end_time = time.time()\n",
    "        print(f\"Generation took {end_time - start_time:.2f} seconds\")\n",
    "        for i, text in enumerate(generated_texts, 1):\n",
    "            print(f\"Sample {i}: {text}\")\n",
    "        \n",
    "        # Try with more creative parameters\n",
    "        print(\"\\nCreative parameters (temp=1.0, top_p=0.95):\")\n",
    "        start_time = time.time()\n",
    "        generated_texts = generate_text_with_mlm(prompt, max_length=50, num_samples=1, top_p=0.95, temperature=1.0)\n",
    "        end_time = time.time()\n",
    "        print(f\"Generation took {end_time - start_time:.2f} seconds\")\n",
    "        for i, text in enumerate(generated_texts, 1):\n",
    "            print(f\"Sample {i}: {text}\")\n",
    "        \n",
    "        # Generate a longer sample\n",
    "        print(\"\\nLonger text generation (temp=0.8, top_p=0.9):\")\n",
    "        start_time = time.time()\n",
    "        generated_texts = generate_text_with_mlm(prompt, max_length=75, num_samples=1, top_p=0.9, temperature=0.8)\n",
    "        end_time = time.time()\n",
    "        print(f\"Generation took {end_time - start_time:.2f} seconds\")\n",
    "        for i, text in enumerate(generated_texts, 1):\n",
    "            print(f\"Sample {i}: {text}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
